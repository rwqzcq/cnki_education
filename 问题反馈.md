
## 5-16
1. 爬取速度太慢了，主要体现在灭一次爬取一篇文献都要打开一次浏览器
> 查询期刊使用selenium，爬取期刊使用beautifulSoup
2. 整和项目文件,都所有的程序都写到一个文件中，然后供虞师兄直接调用
> 这样可维护性太差，查询其他的解决方案，比如类似**shell**的在windows下的可执行文件
> windows系统下有批处理程序可以完成
```bat
@echo off 
start python python文件
```
3. 程序跑了一天都没有获取存储到数据库中
> 原因在于使用selenium框架获取期刊的原文的时候并没有与windows电脑做适配

```python
# paper_parse.py中出现错误
def get_paper_detail_without_content(filename):
    browser = webdriver.Chrome() # mac程序中没有出现
```
## 5-17
1. 期刊中的**日期**中的**期**字去掉

## 5-28
1. 查询期刊的时候翻页太快会提示出现验证码
> 可以选择将

2. gitignore失效
> 清除缓存
> git rm -r --cached .

2. 优化日志管理
> 期刊都是按照年来说的 因此可以选择以年为单位进行爬取 因为除了2019年之外的其他数据都是已知的，因此以年为单位进行爬取和存储，这样可以保证不会一次性将大量数据载入内容，在容错率上也有保证, 在检索词上有修改的时候就再依次再去更新这些log文件。

## 5-30
1. 在主程序中，将更新日志的操作嵌入到了入口程序中，需要将其迁移到**log.py**中

2. 是否还需要写一个将各年的个专题日志转化为一个整的日志文件

## 6-2
1. 在批量插入数据到数据库的过程中，为了确保能够保证插入的数据是唯一的，需要先检索出某一个主题下的所有数据，然后再跟爬取的做一个比较，对两个集合求差集，然后再去插入数据
> https://blog.csdn.net/T1243_3/article/details/80197822

2. 增加单元测试的内容

## 6-3
1. 将日志模块嵌入到当前的系统中，主要是记录一下每一次执行爬取的爬取进度，比如
- 每天每一个主题 每天一共爬取多少条  
- 更新了多少条

> 这个需要考虑使用什么设计模式嵌入到当前的程序中
没有采用设计模式，直接修改了函数，因为涉及到变量的提升与转化，所以修改了函数的返回值

## 6-5
1. 解决日志系统中selenium中混入的问题